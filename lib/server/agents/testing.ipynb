{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama # type: ignore\n",
    "from langchain_groq import ChatGroq # type: ignore\n",
    "from langchain.prompts import ChatPromptTemplate # type: ignore\n",
    "from typing import List, Dict, Any\n",
    "from typing_extensions import TypedDict # type: ignore\n",
    "from langchain_core.output_parsers import StrOutputParser # type: ignore\n",
    "from langgraph.graph import StateGraph, START, END # type: ignore\n",
    "from IPython.display import Image, display # type: ignore\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage # type: ignore\n",
    "from langchain_core.output_parsers import PydanticOutputParser # type: ignore\n",
    "from langchain_text_splitters import TokenTextSplitter # type: ignore\n",
    "import os\n",
    "from dotenv import load_dotenv # type: ignore\n",
    "from typing import List, Dict, Any, Optional\n",
    "import fitz # type: ignore\n",
    "from pydantic import BaseModel, Field # type: ignore\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"LANGSMITH_PROJECT\"] = f\"MineD 2025\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(model=\"llama-3.2-1b-preview\", api_key=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#base model to hold the metadata, and slide summaries that the llm will extract\n",
    "class ResPaperText(BaseModel):\n",
    "    authors: str = Field(..., description=\"List of authors of the research paper\")\n",
    "    title: str = Field(..., description=\"Title of the research paper\")\n",
    "    submission_date: str = Field(..., description=\"Submission date of the research paper\")\n",
    "    keywords: List[str] = Field(..., description=\"List of keywords associated with the research paper\")\n",
    "    references: List[str] = Field(..., description=\"List of references cited in the research paper\")\n",
    "    abstract: str = Field(..., description=\"Abstract of the research paper\")\n",
    "    conclusion: str = Field(..., description=\"Conclusion of the research paper\")\n",
    "    body: List[str] = Field(..., description=\"Body content of the research paper, organized as a list of sections or paragraphs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResPaperExtractState(TypedDict):\n",
    "    pdf_path: Optional[str] = None  # Path to the PDF file\n",
    "    extracted_text: Optional[str] = None  # Full extracted text from the PDF\n",
    "    extracted_images: Optional[List[str]] = None  # Paths to extracted images\n",
    "    slides_content: Optional[List[Dict[str, str]]] = None  # Prepared content for PowerPoint slides\n",
    "    condensed_text: ResPaperText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdf(state: ResPaperExtractState):\n",
    "    pdf_path = state[\"pdf_path\"]\n",
    "    doc = fitz.open(pdf_path)  # Load the PDF only once\n",
    "    \n",
    "    extracted_text = []\n",
    "    extracted_images = []\n",
    "    output_folder = \"extracted_images\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Iterate through each page\n",
    "    for page_number, page in enumerate(doc):\n",
    "        # Extract text\n",
    "        text = page.get_text(\"text\")\n",
    "        extracted_text.append(text)\n",
    "\n",
    "        # Extract images\n",
    "        for img_index, img in enumerate(page.get_images(full=True)):\n",
    "            xref = img[0]\n",
    "            base_image = doc.extract_image(xref)\n",
    "            image_bytes = base_image[\"image\"]\n",
    "            image_ext = base_image[\"ext\"]\n",
    "            img_filename = f\"{output_folder}/page_{page_number+1}_img_{img_index+1}.{image_ext}\"\n",
    "            \n",
    "            with open(img_filename, \"wb\") as img_file:\n",
    "                img_file.write(image_bytes)\n",
    "            \n",
    "            extracted_images.append(img_filename)\n",
    "\n",
    "    # Combine text from all pages\n",
    "    full_text = \"\\n\".join(extracted_text)\n",
    "\n",
    "    # Update state\n",
    "    return {\"extracted_text\": full_text, \"extracted_images\": extracted_images}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "condenser_instruction = \"\"\" \n",
    "You are an AI assistant specialized in processing research papers. \n",
    "\n",
    "Here is the text extracted from a research paper: {extracted_text}\n",
    "\n",
    "When tasked with extracting information from the provided text, follow these guidelines, and structure the content accordingly:\n",
    "1. **Metadata Extraction:** Identify and extract:\n",
    "   - Authors  \n",
    "   - Title  \n",
    "   - Submission Date  \n",
    "   - Keywords  \n",
    "   - References (return as a list) \n",
    "\n",
    "2. **Text Structuring:** Organize the content into:\n",
    "   - Abstract  \n",
    "   - Conclusion  \n",
    "   - Body (as a list of sections or paragraphs)  \n",
    "\n",
    "3. **Slide Summaries:** If applicable, condense key points into structured slide content, ensuring clarity and coherence.\n",
    "\n",
    "Ensure the extracted content is well-structured, concise, and retains essential details.\n",
    "\n",
    "\"\"\"\n",
    "parser = PydanticOutputParser(pydantic_object=ResPaperText)\n",
    "\n",
    "condenser_template = ChatPromptTemplate(\n",
    "   messages=[(\"system\", condenser_instruction),\n",
    "   (\"human\", \"Extract the details from the given text\")],\n",
    "   input_variables=[\"extracted_text\"],\n",
    "   partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "def get_data(state: ResPaperExtractState):\n",
    "    extracted_text = state[\"extracted_text\"]\n",
    "    structured_llm = llm.with_structured_output(ResPaperText)\n",
    "    condenser_prompt = condenser_template.format(extracted_text=extracted_text)\n",
    "    response = structured_llm.invoke(condenser_prompt)\n",
    "    return {\"metadata\": response}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "condenser_instruction = \"\"\" \n",
    "You are an AI assistant specialized in processing research papers. \n",
    "\n",
    "Here is the text extracted from a research paper: {extracted_text}\n",
    "\n",
    "When tasked with extracting information from the provided text, follow these guidelines, and structure the content accordingly:\n",
    "1. **Metadata Extraction:** Identify and extract:\n",
    "   - Authors  \n",
    "   - Title  \n",
    "   - Submission Date  \n",
    "   - Keywords  \n",
    "   - References (return as a list) \n",
    "\n",
    "2. **Text Structuring:** Organize the content into:\n",
    "   - Abstract  \n",
    "   - Conclusion  \n",
    "   - Body (as a list of sections or paragraphs)  \n",
    "\n",
    "3. **Slide Summaries:** If applicable, condense key points into structured slide content, ensuring clarity and coherence.\n",
    "\n",
    "Ensure the extracted content is well-structured, concise, and retains essential details.\n",
    "\n",
    "\"\"\"\n",
    "parser = PydanticOutputParser(pydantic_object=ResPaperText)\n",
    "\n",
    "condenser_template = ChatPromptTemplate(\n",
    "   messages=[(\"system\", condenser_instruction),\n",
    "   (\"human\", \"Given are the extracted details from a research paper, try to make an ppt from the given extracted text, you can add some basic topic from the related format. {extracted_text}\")],\n",
    "   input_variables=[\"extracted_text\"],\n",
    "   partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "def get_ppt_text(state: ResPaperExtractState):\n",
    "    extracted_text = state[\"metadata\"]\n",
    "    structured_llm = llm.with_structured_output(ResPaperText)\n",
    "    condenser_prompt = condenser_template.format(extracted_text=extracted_text)\n",
    "    response = structured_llm.invoke(condenser_prompt)\n",
    "    return {\"metadata\": response}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = StateGraph(ResPaperExtractState)\n",
    "\n",
    "builder.add_node(\"pdf-2-text\", load_pdf)\n",
    "builder.add_node(\"text-condensation\", get_data)\n",
    "builder.add_node(\"make-ppt-text\", get_ppt_text)\n",
    "\n",
    "builder.add_edge(START, \"pdf-2-text\")\n",
    "builder.add_edge(\"pdf-2-text\", \"text-condensation\")\n",
    "builder.add_edge(\"text-condensation\", END)\n",
    "\n",
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "APIStatusError",
     "evalue": "Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.2-1b-preview` in organization `org_01jjve6bv7fhjsqmspe7a3fpmr` service tier `on_demand` on tokens per minute (TPM): Limit 7000, Requested 20071, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAPIStatusError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m state_output \u001b[38;5;241m=\u001b[39m \u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpdf_path\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC:\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mUsers\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mMihir Patel\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mDownloads\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43m1706.03762v7.pdf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Mihir Patel\\.virtualenvs\\mined_2025-9loJUqGE\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:1940\u001b[0m, in \u001b[0;36mPregel.invoke\u001b[1;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, **kwargs)\u001b[0m\n\u001b[0;32m   1938\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1939\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m-> 1940\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1941\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1942\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1943\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1944\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1945\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1946\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1947\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdebug\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdebug\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1948\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1949\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1950\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalues\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1951\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlatest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Mihir Patel\\.virtualenvs\\mined_2025-9loJUqGE\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:1660\u001b[0m, in \u001b[0;36mPregel.stream\u001b[1;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates\u001b[39;00m\n\u001b[0;32m   1656\u001b[0m     \u001b[38;5;66;03m# channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[0;32m   1657\u001b[0m     \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[0;32m   1658\u001b[0m     \u001b[38;5;66;03m# with channel updates applied only at the transition between steps\u001b[39;00m\n\u001b[0;32m   1659\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtick(input_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_channels):\n\u001b[1;32m-> 1660\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1661\u001b[0m \u001b[43m            \u001b[49m\u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtasks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1662\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1663\u001b[0m \u001b[43m            \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1664\u001b[0m \u001b[43m            \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1665\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1666\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[0;32m   1667\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1668\u001b[0m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Mihir Patel\\.virtualenvs\\mined_2025-9loJUqGE\\Lib\\site-packages\\langgraph\\pregel\\runner.py:167\u001b[0m, in \u001b[0;36mPregelRunner.tick\u001b[1;34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[0m\n\u001b[0;32m    165\u001b[0m t \u001b[38;5;241m=\u001b[39m tasks[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 167\u001b[0m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_SEND\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcall\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    175\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[1;32mc:\\Users\\Mihir Patel\\.virtualenvs\\mined_2025-9loJUqGE\\Lib\\site-packages\\langgraph\\pregel\\retry.py:40\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[1;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[0;32m     38\u001b[0m     task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     42\u001b[0m     ns: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "File \u001b[1;32mc:\\Users\\Mihir Patel\\.virtualenvs\\mined_2025-9loJUqGE\\Lib\\site-packages\\langgraph\\utils\\runnable.py:408\u001b[0m, in \u001b[0;36mRunnableSeq.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    404\u001b[0m config \u001b[38;5;241m=\u001b[39m patch_config(\n\u001b[0;32m    405\u001b[0m     config, callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq:step:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    406\u001b[0m )\n\u001b[0;32m    407\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 408\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    409\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    410\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[1;32mc:\\Users\\Mihir Patel\\.virtualenvs\\mined_2025-9loJUqGE\\Lib\\site-packages\\langgraph\\utils\\runnable.py:184\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    183\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, config)\n\u001b[1;32m--> 184\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse:\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "Cell \u001b[1;32mIn[7], line 37\u001b[0m, in \u001b[0;36mget_data\u001b[1;34m(state)\u001b[0m\n\u001b[0;32m     35\u001b[0m structured_llm \u001b[38;5;241m=\u001b[39m llm\u001b[38;5;241m.\u001b[39mwith_structured_output(ResPaperText)\n\u001b[0;32m     36\u001b[0m condenser_prompt \u001b[38;5;241m=\u001b[39m condenser_template\u001b[38;5;241m.\u001b[39mformat(extracted_text\u001b[38;5;241m=\u001b[39mextracted_text)\n\u001b[1;32m---> 37\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mstructured_llm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcondenser_prompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: response}\n",
      "File \u001b[1;32mc:\\Users\\Mihir Patel\\.virtualenvs\\mined_2025-9loJUqGE\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3020\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   3018\u001b[0m context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, config)\n\u001b[0;32m   3019\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 3020\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3021\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3022\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[1;32mc:\\Users\\Mihir Patel\\.virtualenvs\\mined_2025-9loJUqGE\\Lib\\site-packages\\langchain_core\\runnables\\base.py:5352\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   5346\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m   5347\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   5348\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[0;32m   5349\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   5350\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[0;32m   5351\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[1;32m-> 5352\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   5353\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5354\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5355\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5356\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Mihir Patel\\.virtualenvs\\mined_2025-9loJUqGE\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:286\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    277\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    281\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    282\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[0;32m    283\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m    284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[0;32m    285\u001b[0m         ChatGeneration,\n\u001b[1;32m--> 286\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    287\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    288\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    289\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    296\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[1;32mc:\\Users\\Mihir Patel\\.virtualenvs\\mined_2025-9loJUqGE\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:786\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    778\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    779\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    780\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[PromptValue],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    783\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    784\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    785\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Mihir Patel\\.virtualenvs\\mined_2025-9loJUqGE\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:643\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    641\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[0;32m    642\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[1;32m--> 643\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    644\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    645\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[list-item]\u001b[39;00m\n\u001b[0;32m    646\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[0;32m    647\u001b[0m ]\n\u001b[0;32m    648\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[1;32mc:\\Users\\Mihir Patel\\.virtualenvs\\mined_2025-9loJUqGE\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:633\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[0;32m    631\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    632\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m--> 633\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    635\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    636\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    637\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    638\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    639\u001b[0m         )\n\u001b[0;32m    640\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    641\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[1;32mc:\\Users\\Mihir Patel\\.virtualenvs\\mined_2025-9loJUqGE\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:851\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    849\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    850\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 851\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    852\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    853\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    854\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    855\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Mihir Patel\\.virtualenvs\\mined_2025-9loJUqGE\\Lib\\site-packages\\langchain_groq\\chat_models.py:480\u001b[0m, in \u001b[0;36mChatGroq._generate\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    475\u001b[0m message_dicts, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_message_dicts(messages, stop)\n\u001b[0;32m    476\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    477\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    479\u001b[0m }\n\u001b[1;32m--> 480\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessage_dicts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    481\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_result(response)\n",
      "File \u001b[1;32mc:\\Users\\Mihir Patel\\.virtualenvs\\mined_2025-9loJUqGE\\Lib\\site-packages\\groq\\resources\\chat\\completions.py:316\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, n, parallel_tool_calls, presence_penalty, response_format, seed, service_tier, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    194\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m    195\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m    196\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;124;03m    Creates a model response for the given chat conversation.\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;124;03m      timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/openai/v1/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    323\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    324\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    325\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    326\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    327\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_completion_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    328\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    333\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    334\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    335\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    336\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    337\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    338\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    339\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    340\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    341\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    342\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    343\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    344\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    345\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    346\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Mihir Patel\\.virtualenvs\\mined_2025-9loJUqGE\\Lib\\site-packages\\groq\\_base_client.py:1266\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1252\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1253\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1254\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1261\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1262\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1263\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1264\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1265\u001b[0m     )\n\u001b[1;32m-> 1266\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\Mihir Patel\\.virtualenvs\\mined_2025-9loJUqGE\\Lib\\site-packages\\groq\\_base_client.py:958\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    955\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    956\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 958\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    959\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    960\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    961\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    962\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    963\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    964\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Mihir Patel\\.virtualenvs\\mined_2025-9loJUqGE\\Lib\\site-packages\\groq\\_base_client.py:1061\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1058\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m   1060\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1061\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1063\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[0;32m   1064\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1065\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1069\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[0;32m   1070\u001b[0m )\n",
      "\u001b[1;31mAPIStatusError\u001b[0m: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.2-1b-preview` in organization `org_01jjve6bv7fhjsqmspe7a3fpmr` service tier `on_demand` on tokens per minute (TPM): Limit 7000, Requested 20071, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}",
      "\u001b[0mDuring task with name 'text-condensation' and id '13999a50-4721-4358-e19d-7fb7841b0dd0'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "state_output = graph.invoke({\"pdf_path\": \"C:\\\\Users\\\\Mihir Patel\\\\Downloads\\\\1706.03762v7.pdf\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node: pdf_path\n",
      "C:\\Users\\milap\\OneDrive\\Desktop\\CLG\\3rd YR\\SEM VI\\mined_2025\\lib\\server\\Milap_Tathya_ICC_June_2025.pdf\n",
      "Node: extracted_text\n",
      "ConvNeXt-based Multi-Class Hydrocarbon Spill\n",
      "Classification in Hyperspectral Imagery\n",
      "Milap Patel, Tathya Patel, Anuja Nair, Member, IEEE, Tarjni Vyas, Shivani Desai,\n",
      "Sudeep Tanwar, Senior Member, IEEE\n",
      "Department of Computer Science and Engineering, School of Technology, Nirma University, Ahmedabad, Gujarat, India\n",
      "Emails: 22bce186@nirmauni.ac.in, 22bce352@nirmauni.ac.in, anuja.nair@nirmauni.ac.in,\n",
      "tarjni.vyas@nirmauni.ac.in, shivani.desai@nirmauni.ac.in, sudeep.tanwar@nirmauni.ac.in\n",
      "AbstractThis paper proposes a new approach of hydrocarbon\n",
      "spill detection using hyperspectral imaging (HSI) and fine-tuning\n",
      "ConvNeXt convolutional neural network (CNN). Hydrocarbon\n",
      "spill hyperspectral dataset (HSHD) containing 124 HSIs into four\n",
      "classes-cleans, gasoline, motor oil, and thinner is used in the\n",
      "training as well as testing phase. To overcome the computational\n",
      "complexity associated with the high spatial dimensions of HSIs\n",
      "(1024  1024  20), instead of resizing, each image is divided\n",
      "into 16 smaller patches of size 256  256  20 to ensure that no\n",
      "critical spatial-spectral information is lost. The ConvNeXt model\n",
      "is adapted for 20 spectral channels and has its classification head\n",
      "modified for multi-class prediction. This patch-based approach,\n",
      "coupled with the models spectral-spatial learning capabilities,\n",
      "allows for accurate classification with minimal misclassifications,\n",
      "as shown by the confusion matrix. The proposed framework\n",
      "underlines the efficacy of deep learning (DL) in hyperspectral\n",
      "data analysis, offering significant advantages for environmental\n",
      "monitoring and rapid hydrocarbon spill identification.\n",
      "Index TermsHydrocarbon spill, ConvNeXt, CNN, spectral-\n",
      "spatial, hyperspectral imaging\n",
      "I. INTRODUCTION\n",
      "Oil spill and hydrocarbon spill is one of the significant\n",
      "environmental challenges, causing adverse and harmful effects\n",
      "on marine ecosystems and coastal communities. Such a release\n",
      "of hydrocarbons into water not only puts aquatic organisms at\n",
      "risk but impacts human activities related to these ecosystems.\n",
      "Oil spill detection, which in the past could only be based on\n",
      "visual analysis or restricted by remote sensing technologies,\n",
      "may be inefficient and time-consuming, as it might not even\n",
      "guarantee the exact nature of the oil involved. On the other\n",
      "hand, hyperspectral imaging (HSI) is advantageous since it\n",
      "captures more spectral information with high spatial coverage\n",
      "in various wavelength ranges; the data is thus useful in dis-\n",
      "tinguishing and determining types of oils. This speeds up and\n",
      "becomes effective in terms of responding to oil spills [1].\n",
      "HSI are rich information sources, which capture spectral data\n",
      "over hundreds of contiguous wavelength bands and go beyond\n",
      "the visible spectrum to the near-infrared and other regions.\n",
      "Traditional RGB images are a representation of only three\n",
      "bands-red, green, and blue-but in the case of HSIs, the spectral\n",
      "signature is highly detailed for each pixel in the spatial domain.\n",
      "This high spectral resolution allows the differentiation and\n",
      "identification of materials and substances through the patterns\n",
      "they give off in their unique reflectance characteristics, making\n",
      "HSIs extremely valuable in a number of applications, including\n",
      "environmental monitoring, mineral exploration, and chemical\n",
      "spill detection. HSI taps both the spectral and spatial informa-\n",
      "tion available and thus provides powerful means for accurate\n",
      "classification and analysis in complex real-world situations.\n",
      "Many researchers have worked in the field of hydrocardon\n",
      "spill classification over the years. Sandhiya et al. [2] proposed\n",
      "an application of machine learning (ML) techniques for oil spill\n",
      "detection using satellite and drone imagery. They highlighted\n",
      "the significant role that the synthetic aperture radar (SAR)\n",
      "technology plays in identifying and monitoring oil pollution in\n",
      "marine environments. They trained ML methods like support\n",
      "vector machine (SVM), decision tree (DT), linear regression\n",
      "(LR) on labeled datasets comprising images of clean water\n",
      "and oil-contaminated water. Moving further, Sherif et al. [3]\n",
      "implemented a deep learning (DL) approach using an artificial\n",
      "neural network (ANN). The model was trained on processed\n",
      "satellite image data, employing techniques like gradient descent\n",
      "to minimize prediction errors. Bui et al. [4] presented another\n",
      "solution with data augmentation and attention mechanism.\n",
      "They used a tailored data augmentation strategy leveraging a\n",
      "conditional generative adversarial network (GAN), specifically\n",
      "the Pix2Pix framework was implemented to generate images\n",
      "that would mimic real oil spills to enhance the diversity of the\n",
      "training dataset. Then, a dual attention mechanism-based DL\n",
      "model was employed, integrating spatial and channel attention\n",
      "modules to boost oil spill classification accuracy. Yang et\n",
      "al. [5] studied high spectral resolution from HSI data and\n",
      "thermal infrareds sensitivity to temperature differences for\n",
      "identifying oil types. They focused on crude oil, emulsions, and\n",
      "refined products, collecting data using airborne HSI sensors and\n",
      "thermal cameras. The study employed SVM, RF, and convolu-\n",
      "tional neural network (CNN), for classification and found that\n",
      "combining modalities improved recognition accuracy.\n",
      "Based on the research gaps found in state-of-the-art ap-\n",
      "proaches, this paper proposes a new approach of hydrocarbon\n",
      "spill detection using HSI and fine-tuning ConvNeXt CNN. Hy-\n",
      "drocarbon spill hyperspectral dataset (HSHD) containing 124\n",
      "HSIs into four classes-cleans, gasoline, motor oil, and thinner\n",
      "is used in the training as well as testing phase. To overcome\n",
      "the computational complexity associated with the high spatial\n",
      "dimensions of HSIs (1024102420), instead of resizing, each\n",
      "image is divided into 16 smaller patches of size 25625620\n",
      "to ensure that no critical spatial-spectral information is lost. The\n",
      "\n",
      "ConvNeXt model is adapted for 20 spectral channels and has\n",
      "its classification head modified for multi-class prediction. This\n",
      "patch-based approach, coupled with the models spectral-spatial\n",
      "learning capabilities, allows for accurate classification with\n",
      "minimal misclassifications, as shown by the confusion matrix.\n",
      "The proposed framework underlines the efficacy of DL in HSI\n",
      "data analysis, offering significant advantages for environmental\n",
      "monitoring and rapid hydrocarbon spill identification.\n",
      "A. Motivation\n",
      "1) Oil spills or hydrocarbon spills can severely damage\n",
      "marine ecosystems or agricultural lands sometimes. Early\n",
      "detection using HSI is important for quick response and\n",
      "minimal environmental damage.\n",
      "2) Traditional detection methods do not have the accuracy\n",
      "and spectral resolution required in changing conditions.\n",
      "HSI provides superior accuracy in the identification and\n",
      "quantification of oil spills.\n",
      "3) Recent developments in satellite and HSI sensor technol-\n",
      "ogy improve remote sensing ability, covering huge areas\n",
      "with oil spill monitoring very efficiently, so it finds usage\n",
      "in remedial actions.\n",
      "B. Research Contributions\n",
      "This paper has the following research contributions.\n",
      "1) We utilise the HSHD dataset, which provides detailed\n",
      "HSI data to enhance the models ability to accurately\n",
      "classify oil spills.\n",
      "2) We propose a DL-based approach for detecting oil and\n",
      "hydrocarbon spills by fine-tuning various state-of-art\n",
      "CNN architectures, leveraging their strengths in feature\n",
      "extraction from HSI.\n",
      "3) We demonstrate the models predictive accuracy using\n",
      "various performance metrics, such as accuracy, precision,\n",
      "recall and F1-score, to evaluate its results .\n",
      "C. Paper Organization\n",
      "The paper is further organized as follows: Section II proposes\n",
      "the system model and the problem formulation, Section III\n",
      "explains the proposed framework, Section IV discusses the\n",
      "results, and Section V provides the conclusion and future work.\n",
      "II. SYSTEM MODEL AND PROBLEM FORMULATION\n",
      "A. System Model\n",
      "The system model of the proposed framework is illustrated\n",
      "in Fig. 1. The detection of oil spills using HSI data leverages\n",
      "advanced remote sensing and DL technologies. The system\n",
      "operates through a multi-step pipeline, starting with data ac-\n",
      "quisition via HSI sensors mounted on satellites and drones. Let\n",
      "X RHW C represent the set of HSIs captured, where H\n",
      "and W denote the height and width of the images, respectively,\n",
      "and C is the number of spectral channels. Each image is\n",
      "segmented into smaller patches {Pi}, where Pi RhwC,\n",
      "with h and w representing the height and width of the patches,\n",
      "respectively. This segmentation facilitates localized analysis and\n",
      "reduces the computational overhead associated with processing\n",
      "large images. After data acquisition, the HSI data is transmitted\n",
      "to centralized data centers via secure communication channels\n",
      "for preprocessing and analysis. Let X  = {P \n",
      "i} denote the\n",
      "set of preprocessed patches. Each patch is then fed into the\n",
      "DL model for predictive analysis. The processed insights are\n",
      "subsequently communicated to relevant stakeholders, such as\n",
      "disaster management teams and local rescue operators, en-\n",
      "abling timely remedial actions. The primary objective of the\n",
      "system is to classify each patch {Pi} into its respective class\n",
      "yi Y. A DL-based approach is employed, fine-tuning various\n",
      "CNN architectures for feature extraction and classification.\n",
      "The models are optimized using the AdamW optimizer and\n",
      "CrossEntropyLoss function, with performance metrics such as\n",
      "accuracy, precision, recall, and F1-score used to evaluate their\n",
      "effectiveness.\n",
      "Fig. 1: System model.\n",
      "B. Problem Formulation\n",
      "Let the hyperspectral dataset be as follows:\n",
      "D = {(P \n",
      "i, yi)}N\n",
      "i=1,\n",
      "(1)\n",
      "where N is the total number of patches, P \n",
      "i\n",
      "RhwC\n",
      "represents the ith preprocessed patch of spatial dimensions\n",
      "hw with C spectral channels, and yi Y is its corresponding\n",
      "class label. The dataset comprises patches derived from high-\n",
      "resolution HSIs, where each patch inherits the label of its parent\n",
      "image, ensuring the preservation of spatial and spectral infor-\n",
      "mation. The objective is to train a DL model f, parameterized\n",
      "by , that maps an input patch P \n",
      "i to its predicted label yi.\n",
      "Mathematically, this can be expressed as:\n",
      "f(P \n",
      "i) = yi,\n",
      "i {1, 2, . . . , N},\n",
      "(2)\n",
      "where yi Y is the predicted label for the ith patch. The\n",
      "training process involves minimizing the CrossEntropyLoss L,\n",
      "which measures the discrepancy between the true labels yi and\n",
      "the predicted probabilities for each class. The loss function is\n",
      "defined as:\n",
      "L() = 1\n",
      "N\n",
      "N\n",
      "X\n",
      "i=1\n",
      "K\n",
      "X\n",
      "k=1\n",
      "1(yi = k) log(p(yi = k | P \n",
      "i))\n",
      "(3)\n",
      "where:\n",
      "\n",
      " K is the total number of classes,\n",
      " p(yi = k | P \n",
      "i) represents the predicted probability for\n",
      "class k,\n",
      " 1() is an indicator function that evaluates to 1 if yi = k,\n",
      "otherwise 0.\n",
      "The primary objective is to optimize the model parameters \n",
      "such that the loss function L() is minimized. This optimization\n",
      "problem can be formally expressed as:\n",
      "= arg min\n",
      "\n",
      "L(),\n",
      "(4)\n",
      "where represents the set of parameters that minimizes the\n",
      "average loss across all training samples.\n",
      "By minimizing L(), the model learns to accurately cap-\n",
      "ture the spatial and spectral patterns within the HSI data,\n",
      "enabling robust classification across all classes. The patch-\n",
      "based approach ensures that the large spatial dimensions of the\n",
      "original images do not impose excessive computational burdens\n",
      "while preserving critical spatial-spectral details necessary for\n",
      "accurate classification. Additionally, the use of CrossEntropy-\n",
      "Loss ensures the model effectively handles imbalances in class\n",
      "distributions, promoting stable and efficient learning.\n",
      "III. THE PROPOSED FRAMEWORK\n",
      "The proposed framework, as demonstrated in Fig. 2. It is\n",
      "a three-layered approach containing the data collection layer,\n",
      "intelligent layer, and application layer.\n",
      "Fig. 2: Proposed framework.\n",
      "A. Data Collection Layer\n",
      "The data collection layer forms the base of the framework,\n",
      "using satellite and drone platforms equipped with HSI cameras\n",
      "to collect raw data in a number of electromagnetic spectra.\n",
      "HSI is fundamental in oil spill detection, since it provides\n",
      "detailed spectral information that differentiates oil from water,\n",
      "vegetation, and other substances due to its high spectral res-\n",
      "olution. Recent satellite missions, such as ESAs Copernicus\n",
      "program with Sentinel satellites and the upcoming EnMAP,\n",
      "provide high-resolution HSI data over large ocean areas. The\n",
      "drones complement these satellites, being flexible and of higher\n",
      "spatial resolution, thus enabling fast, localized data acquisition\n",
      "with advanced HSI cameras. These are then transmitted to the\n",
      "centralized data center via satellite links or through 4G/5G\n",
      "networks, depending on the location of the platform. The in-\n",
      "frastructure will provide real-time data ingestion, preprocessing,\n",
      "and storage of data, readying the data for further analysis by\n",
      "the intelligence layer for effective oil spill classification.\n",
      "B. Intelligence Layer\n",
      "1) Data Preprocessing: The HSHD [8] is a specific set of\n",
      "124 HSIs to classify four different classes: clean samples (un-\n",
      "contaminated), and samples contaminated with gasoline, motor\n",
      "oil, or thinner. Every HSI has a resolution of 1024102420,\n",
      "where the first two dimensions are the spatial resolution, and\n",
      "the third dimension is the spectral channels. The dataset is\n",
      "stored in standard ENVI format, where each sample will have\n",
      "two accompanying files, such as the header metadata (.hdr)\n",
      "that provides wavelength information as well as the spatial\n",
      "resolution; the .dat file will carry the HSI data cube. To\n",
      "load images, Spectral Python (SPy) [9] was used: it enables\n",
      "transformation of ENVI files into appropriate numpy arrays\n",
      "which are more tractable for calculation.\n",
      "Once loaded, the images undergo normalization along the\n",
      "spatial dimensions to ensure consistent scaling and facilitate\n",
      "convergence during model training. Due to the large spatial\n",
      "resolution of 10241024, training on full-sized images is com-\n",
      "putationally expensive and risks exceeding available hardware\n",
      "memory. However, directly reducing the spatial dimensions\n",
      "through downsampling can degrade performance by discarding\n",
      "critical spatial information, which is essential for accurate\n",
      "classification. To address this, each HSI is split into 16 smaller\n",
      "patches of 256  256  20, with each patch inheriting the\n",
      "same label as the original image as shown in Fig. 3. This\n",
      "method does not lose any spatial information but increases the\n",
      "size of the training dataset, which helps mitigate overfitting by\n",
      "exposing the model to more variations during training. Lastly,\n",
      "these processed images are converted to tensors so that they\n",
      "can be used to train the models implemented in the PyTorch\n",
      "deeplearning framework . This preprocessing pipeline balances\n",
      "computational feasibility, meanwhile retaining rich spatial and\n",
      "spectral details required for optimal training of the ConvNeXt\n",
      "model.\n",
      "2) Model Architecture and Training: The ConvNeXt model\n",
      "[10] was adapted and fine-tuned to classify HSIs into four\n",
      "classes: clean, gasoline, motor oil, and thinner. HSIs with\n",
      "20 spectral channels were input to the model, necessitating\n",
      "modifications to the initial convolution layer to handle the 20-\n",
      "channel input. This was accomplished by replacing the stem\n",
      "cell with a 4  4 convolution with a stride of 4, which means\n",
      "it downsamples the input dimensions by a factor of 4  4 and\n",
      "projects the 20 channels into 96 feature maps. The stages in\n",
      "ConvNeXt consist of depthwise convolutions for spatial feature\n",
      "extraction, inverted bottleneck blocks with an expansion ratio\n",
      "of 4, and 1  1 convolutions for channel mixing. It represents\n",
      "\n",
      "Fig. 3: A sample image of type clean (no contamination) from\n",
      "the dataset. The 11th, 8th and 4th band were selected as the\n",
      "red, green and blue components respectively, to display the HSI\n",
      "as RGB image. The image is divided into 16 patches.\n",
      "the spatial feature extraction at every stage as follows:\n",
      "Yi,h,w =\n",
      "kh\n",
      "X\n",
      "u=1\n",
      "kw\n",
      "X\n",
      "v=1\n",
      "Wi,u,vXi,h+u,w+v + bi,\n",
      "(5)\n",
      "where kh and kw denote the kernel dimensions, W and b are the\n",
      "learnable weights and biases, and h, w index spatial dimensions.\n",
      "Depthwise convolutions separately process each input channel,\n",
      "reducing computational overhead.\n",
      "ConvNeXt incorporates inverted bottleneck blocks, where the\n",
      "hidden dimension of the multi-layer perceptron (MLP) block is\n",
      "expanded by a factor of 4:\n",
      "H = GELU(W1H + b1),\n",
      "Y = W2H + b2,\n",
      "(6)\n",
      "where W1 Rdd, W2 Rdd, and d is the expanded\n",
      "dimension. Layer normalization (LN) replaces batch normal-\n",
      "ization (BN) to stabilize training:\n",
      "LN(X) = X \n",
      "\n",
      "2 + \n",
      "  + ,\n",
      "(7)\n",
      "where  and 2 are the mean and variance of the input features,\n",
      "and ,  are learnable parameters. Gaussian error linear unit\n",
      "(GELU) activation further smooths nonlinear transformations:\n",
      "GELU(x) = x  (x),\n",
      "(x) = 1\n",
      "2\n",
      "\u0014\n",
      "1 + erf\n",
      "\u0012 x\n",
      "\n",
      "2\n",
      "\u0013\u0015\n",
      ".\n",
      "(8)\n",
      "The classification head was adjusted to output logits for the\n",
      "four target classes via a fully connected layer, minimizing the\n",
      "cross-entropy loss:\n",
      "L = 1\n",
      "N\n",
      "N\n",
      "X\n",
      "i=1\n",
      "4\n",
      "X\n",
      "c=1\n",
      "I[yi = c] log p(c|xi),\n",
      "(9)\n",
      "where p(c|xi) denotes the predicted probability for class c, and\n",
      "I[yi = c] is an indicator function. The model was trained for\n",
      "20 epochs using the AdamW optimizer with a weight decay\n",
      "of 102. The learning rate was initialized at 104 and reduced\n",
      "by a factor of 0.1 via a ReduceLROnPlateau scheduler if the\n",
      "test loss did not improve for three consecutive epochs. Early\n",
      "stopping with a patience of 5 epochs was employed to prevent\n",
      "overfitting.\n",
      "Algorithm 1 Oil Spill Detection using HSI and ConvNeXt\n",
      "1: Input: Hyperspectral Dataset\n",
      "2: procedure OIL SPILL DETECTOR(Hyperspectral Dataset,\n",
      "ConvNeXt)\n",
      "3: Data preprocessing:\n",
      "4: Create HyperspectralDataset class with following parame-\n",
      "ters:\n",
      "5: patch size = 512\n",
      "6: image size = 1024  1024  20 channels\n",
      "7: for each Image Hyperspectral Dataset do\n",
      "8:\n",
      "patches ExtractPatches(Image, patch size)\n",
      "9:\n",
      "normalized patches NormalizeSpectrum(patches)\n",
      "10: end for\n",
      "11: train dataset, test dataset Split(dataset, [0.7, 0.3])\n",
      "12: dataloaders CreateDataLoader(batch size = 16)\n",
      "13: define ConvNeXt model:\n",
      "14: model ConvNeXt Small()\n",
      "15: input layer Conv2d(in channels=20, out channels=96)\n",
      "16: output layer Linear(out features=4)\n",
      "17: Training Configuration:\n",
      "18: optimizer AdamW(lr=0.0001, weight decay=0.01)\n",
      "19: scheduler ReduceLROnPlateau(patience=3, factor=0.1)\n",
      "20: loss function CrossEntropyLoss()\n",
      "21: for epoch in range(max epochs) do\n",
      "22:\n",
      "Training Phase:\n",
      "23:\n",
      "for each batch train dataloader do\n",
      "24:\n",
      "predictions model(batch)\n",
      "25:\n",
      "loss loss function(predictions, labels)\n",
      "26:\n",
      "Backward propagation and optimize\n",
      "27:\n",
      "end for\n",
      "28:\n",
      "Evaluation Phase:\n",
      "29:\n",
      "for each batch test dataloader do\n",
      "30:\n",
      "predictions model(batch)\n",
      "31:\n",
      "Calculate metrics (F1, Precision, Recall, AUROC)\n",
      "32:\n",
      "end for\n",
      "33:\n",
      "Update learning rate based on validation loss\n",
      "34:\n",
      "Check early stopping condition (patience = 5)\n",
      "35: end for\n",
      "36: Return trained model for oil spill classification\n",
      "C. Application Layer\n",
      "The application layer communicates processed data from the\n",
      "intelligence layer to disaster management authorities, translat-\n",
      "ing HSI insights into actionable responses. Upon identifying oil\n",
      "spills with high precision, real-time information is communi-\n",
      "cated for the quick coordination of containment and remediation\n",
      "\n",
      "actions. Modern disaster management systems rely on real-time\n",
      "data feeds from environmental monitoring platforms to support\n",
      "dynamic decision-making. Advanced tools, such as geographic\n",
      "information systems (GIS) and decision support systems (DSS),\n",
      "help in visualizing the impacts of oil spills, forecasting en-\n",
      "vironmental and economic damage. From this classification\n",
      "data, disaster management teams can initiate a unified response:\n",
      "deployment of containment booms, skimmers, and dispersants,\n",
      "mobilization of cleanup crews and notify affected stakeholders.\n",
      "This integrated approach, using HSI data and advanced tools,\n",
      "enhances the ability to mitigate environmental damage, protect\n",
      "ecosystems, and reduce economic losses.\n",
      "IV. RESULTS AND DISCUSSION\n",
      "A. Experimental Setup\n",
      "The proposed framework is implemented on Kaggles code\n",
      "editor. For computational purposes, the Tesla T4 TPU is used.\n",
      "It is equipped with 2,560 CUDA cores and 16GB of GDDR6\n",
      "memory, thus allowing for faster and more efficient processing.\n",
      "We implemented the model using Python version 3.10.14,\n",
      "along with essential libraries such as NumPy version 1.26.4\n",
      "for numerical computations, Pandas version 2.2.3 for data\n",
      "manipulation and preprocessing, Matplotlib version 3.7.5 for\n",
      "data visualization, PyTorch version 2.4.0 for implementing the\n",
      "DL model, and Scikit-learn version 1.2.2 for additional ML\n",
      "functionalities.\n",
      "Fig. 4: Test time accuracy and F1-score comparison between\n",
      "different CNN models.\n",
      "B. Performance Analysis and Discussion\n",
      "Fig. 4 shows the comparative analysis of the four models\n",
      "demonstrating the best performance of ConvNeXt for the given\n",
      "task of classifying the oil spills from the dataset with a test\n",
      "accuracy of 96.93 % and an F1 score of 0.97, which leads\n",
      "to good generalization with balanced precision and recall, thus\n",
      "performing particularly well in identifying spill and non-spill\n",
      "regions correctly. AlexNet performed well, with a test accuracy\n",
      "of 92.82 % and an F1 score of 0.92, which indicates its\n",
      "capability as a simpler yet reliable architecture. EfficientNet\n",
      "v2 achieved moderate results, with a test accuracy of 90.6 %\n",
      "and an F1 score of 0.91, while ResNet50 showed the lowest\n",
      "performance among the models, with a test accuracy of 89.22%\n",
      "and an F1 score of 0.89. These results point to the superiority\n",
      "of ConvNeXt in the ability to tackle complex patterns in\n",
      "hydrocarbon spill detection, thereby suggesting suitability for\n",
      "deployment in real-world applications.\n",
      "The superior performance of ConvNeXt can be attributed\n",
      "to its modern architecture that has the advancements from\n",
      "vision transformers (ViT) [11] with simplicity from CNNs.\n",
      "This hybrid design makes it possible for ConvNeXt to capture\n",
      "both global and local features, which are very important to\n",
      "differentiate between hydrocarbon spills and the background\n",
      "noise in high-resolution images. Moreover, the hierarchical\n",
      "feature extraction in the architecture of ConvNeXt is suitable\n",
      "for data with variable spill patterns and sizes. It should be\n",
      "noticed that all models were trained with the same batch size,\n",
      "which was 16 and patch size, which was 256, and other training\n",
      "parameters, as mentioned in III-B2. Thus, comparison should\n",
      "be fair. ConvNeXts architectural improvements and effective\n",
      "feature representation would have been key factors to justify\n",
      "the superiority by this model.\n",
      "Then, Fig. 5 describes the training and test performance of\n",
      "(a)\n",
      "(b)\n",
      "Fig. 5: Training vs test for (a) model accuracy and (b) loss over\n",
      "the epochs for oil spill detection.\n",
      "the proposed ConvNext model on the given task in terms of\n",
      "accuracy and loss. The reduction in both train and test loss\n",
      "over the epochs (Fig. 5b) shows that the model was able to\n",
      "learn well from the dataset, and the overall training process is\n",
      "smooth. This can be credited to the normalized input images,\n",
      "and weight-regularized AdamW optimizer. The test loss is\n",
      "\n",
      "slightly higher than train loss, indicating that the model is\n",
      "slightly overfitting as the epochs increases. one can deduce the\n",
      "robustness of the model from the increasing accuracy of the\n",
      "model over the epochs (Fig. 5a).\n",
      "Fig. 6 shows the confusion matrix for the classification\n",
      "Fig. 6: Confusion matrix for ConvNeXt.\n",
      "performance of the proposed fine-tuned ConvNeXt model on\n",
      "the HSHD. The model shows an overall high accuracy in\n",
      "all four classes: clean, gasoline, motor oil, and thinner, with\n",
      "strong diagonal values. In particular, the model shows a perfect\n",
      "classification rate for the clean class (class 0) with an accuracy\n",
      "of 100%. Moreover, class 2 with motor oil shows a very\n",
      "good correctness level of 99%, making almost all samples\n",
      "get well-classified. Class 1 with gasoline indicates a clear-cut\n",
      "classification with 93% accuracy, though there is a small portion\n",
      "of misclassification as thinner oil (class 3). Class 3 or thinner\n",
      "stands robustly at 95%, causing some minor misclassifications\n",
      "into the gasoline and motor oil classes, respectively.\n",
      "As depcted from Fig. 6 the model is very efficient in distin-\n",
      "guishing the clean class (class 0) from contaminated samples,\n",
      "which is crucial in practical scenarios for rapid detection of\n",
      "uncontaminated areas. Second, the high accuracy for motor\n",
      "oil (class 2) indicates that the spectral features of this class\n",
      "are distinct enough for the model to classify them with near-\n",
      "perfect precision. However, the slight misclassifications ob-\n",
      "served between gasoline (class 1) and thinner (class 3) suggest\n",
      "that these two hydrocarbons share some overlapping spectral\n",
      "characteristics, which might make them harder to differentiate.\n",
      "This overlap might be due to similarities in their chemical\n",
      "compositions.\n",
      "V. CONCLUSION\n",
      "In this work, we proposed a deep DL-based framework for\n",
      "hydrocarbon spill detection using HSI data. We leveraged the\n",
      "ConvNeXt CNN adapted to process 20 spectral channels, and\n",
      "classifying four classes.We divided large HSIs into smaller\n",
      "patches instead of resizing them. This approach ensured that\n",
      "critical spatial-spectral features were preserved and computa-\n",
      "tional constraints could be efficiently managed. This patch-\n",
      "based method reduces the probability of information loss while\n",
      "enhancing model generalization performance across complex\n",
      "HSI datasets. The proposed approach attained high accuracy\n",
      "classification performance with four classes of hydrocarbon\n",
      "contamination, indicating high robustness and reliability. We\n",
      "showed the robustness of the ConvNext model by comparing\n",
      "it with other state-of-the-art CNN models like EfficientNet-\n",
      "V2, AlexNet, and Resnet50. Thus, the overall findings have\n",
      "highlighted the applicability of fine-tuned CNNs to HSI data\n",
      "analysis with many practical implications to environmental\n",
      "monitoring and disaster response systems. Future work could\n",
      "include integrating additional datasets, domain-specific aug-\n",
      "mentation techniques, and real-world deployment to enhance\n",
      "the models applicability and performance.\n",
      "REFERENCES\n",
      "[1] A. Bhargava, A. Sachdeva, K. Sharma, M. H. Alsharif, P. Uthansakul, and\n",
      "M. Uthansakul, Hyperspectral imaging and its applications: A review,\n",
      "Heliyon, vol. 10, no. 12, p. e33208, 2024.\n",
      "[2] S. S, R. R, S. M, and V. S, Detection of oil spill events at sea using\n",
      "machine learning, in 2023 5th International Conference on Inventive\n",
      "Research in Computing Applications (ICIRCA), pp. 5358, 2023.\n",
      "[3] K. Sherif, F. H. Rizk, A. M. Zaki, M. M. Eid, N. Khodadadi, A. Ibrahim,\n",
      "A. A. Abdelhamid, L. Abualigah, and E.-S. M. El-Kenawy, Revolution-\n",
      "izing oil spill detection: A machine learning approach for satellite image\n",
      "classification, in 2024 International Telecommunications Conference\n",
      "(ITC-Egypt), pp. 245250, 2024.\n",
      "[4] N. A. Bui, Y. Oh, and I. Lee, Oil spill detection and classification through\n",
      "deep learning and tailored data augmentation, International Journal of\n",
      "Applied Earth Observation and Geoinformation, vol. 129, p. 103845,\n",
      "2024.\n",
      "[5] J. Yang, Y. Hu, J. Zhang, Y. Ma, Z. Li, and Z. Jiang, Identification\n",
      "of marine oil spill pollution using hyperspectral combined with thermal\n",
      "infrared remote sensing, Frontiers in Marine Science, vol. 10, 2023.\n",
      "[6] J. M. Haut, S. Moreno-Alvarez, R. Pastor-Vargas, A. Perez-Garcia, and\n",
      "M. E. Paoletti, Cloud-based analysis of large-scale hyperspectral imagery\n",
      "for oil spill detection, IEEE Journal of Selected Topics in Applied Earth\n",
      "Observations and Remote Sensing, vol. 17, pp. 24612474, 2024.\n",
      "[7] S. Jia, S. Jiang, Z. Lin, N. Li, M. Xu, and S. Yu, A survey: Deep\n",
      "learning for hyperspectral image classification with few labeled samples,\n",
      "Neurocomputing, vol. 448, pp. 179204, 2021.\n",
      "[8] D. Rivas-Lalaleo and C. Hernandez, Hydrocarbon spill hyperspectral\n",
      "dataset (hshd, 2024.\n",
      "[9] Spectral Python Development Team, Spectral python (spy). Accessed:\n",
      "2025-01-19.\n",
      "[10] Z. Liu, H. Mao, C.-Y. Wu, C. Feichtenhofer, T. Darrell, and S. Xie, A\n",
      "convnet for the 2020s, 2022.\n",
      "[11] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Un-\n",
      "terthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit,\n",
      "and N. Houlsby, An image is worth 16x16 words: Transformers for image\n",
      "recognition at scale, 2021.\n",
      "\n",
      "Node: extracted_images\n",
      "['extracted_images/page_2_img_1.png', 'extracted_images/page_3_img_1.png', 'extracted_images/page_4_img_1.png', 'extracted_images/page_5_img_1.png', 'extracted_images/page_5_img_2.png', 'extracted_images/page_5_img_3.png', 'extracted_images/page_6_img_1.png']\n"
     ]
    }
   ],
   "source": [
    "for key, value in state_output.items():\n",
    "    print(f\"Node: {key}\")\n",
    "    if isinstance(value, str) or isinstance(value, list):\n",
    "        print(value)\n",
    "    else:\n",
    "        print(state_output[key])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mined_2025-9loJUqGE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
